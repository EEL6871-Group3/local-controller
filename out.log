nohup: ignoring input
2023-11-28 20:21:33,985 - root - DEBUG - Sample Rate: 5 seconds
2023-11-28 20:21:33,985 - root - DEBUG - Reference Input (CPU Usage): 0.8%
2023-11-28 20:21:33,985 - root - DEBUG - Job Sleep Time: 15 seconds
2023-11-28 20:21:33,985 - root - DEBUG - Job File Name: job_list.txt
2023-11-28 20:21:33,985 - root - DEBUG - CPU API Endpoint: http://localhost:5001/cpu
2023-11-28 20:21:33,985 - root - DEBUG - Pod Number API Endpoint: http://localhost:5001/pod-num
2023-11-28 20:21:33,985 - root - DEBUG - Create Pod API Endpoint: http://localhost:5001/pod
2023-11-28 20:21:33,985 - root - DEBUG - Maximum Number of Pods: 1
2023-11-28 20:21:33,985 - root - INFO - start close loop
2023-11-28 20:21:33,987 - root - INFO - getting job list from job_list.txt
2023-11-28 20:21:34,346 - root - INFO - current CPU: 0.0143815200625
2023-11-28 20:21:34,556 - root - INFO - max_pod != pod_num, skipping closed loop
2023-11-28 20:21:34,556 - root - INFO - current max_pod: 1
2023-11-28 20:21:38,992 - root - INFO - getting job list sucess, start rendering jobs
2023-11-28 20:21:38,994 - root - INFO - saving CPU 0.0143815200625 and max_pod 1
 * Serving Flask app 'local_controller'
 * Debug mode: off
2023-11-28 20:21:38,998 - werkzeug - INFO - [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on http://127.0.0.1:5004
2023-11-28 20:21:38,998 - werkzeug - INFO - [33mPress CTRL+C to quit[0m
2023-11-28 20:21:39,049 - root - INFO - scheduling job stress-ng --io 1 --vm 1 --vm-bytes 2G --timeout 158s
2023-11-28 20:21:39,105 - root - INFO - job scheduled
2023-11-28 20:21:39,952 - root - INFO - current CPU: 0.0143815200625
2023-11-28 20:21:40,069 - root - INFO - last jobst started 0.963729s ago, skipping closed loop
2023-11-28 20:21:40,069 - root - INFO - current max_pod: 1
2023-11-28 20:21:44,001 - root - INFO - saving CPU 0.0143815200625 and max_pod 1
2023-11-28 20:21:45,408 - root - INFO - current CPU: 0.015533353187499998
2023-11-28 20:21:45,480 - root - INFO - last jobst started 6.374786s ago, skipping closed loop
2023-11-28 20:21:45,480 - root - INFO - current max_pod: 1
2023-11-28 20:21:49,004 - root - INFO - saving CPU 0.015533353187499998 and max_pod 1
2023-11-28 20:21:50,819 - root - INFO - current CPU: 0.015533353187499998
2023-11-28 20:21:50,919 - root - INFO - setting max_pod: 0
2023-11-28 20:21:54,009 - root - INFO - saving CPU 0.015533353187499998 and max_pod 1
2023-11-28 20:21:54,197 - root - INFO - current pod num: 1, max pod num: 1, job not scheduled
2023-11-28 20:21:56,346 - root - INFO - current CPU: 0.015533353187499998
2023-11-28 20:21:56,411 - root - INFO - setting max_pod: 2
2023-11-28 20:21:59,015 - root - INFO - saving CPU 0.015533353187499998 and max_pod 2
2023-11-28 20:22:01,788 - root - INFO - current CPU: 0.07681022475
2023-11-28 20:22:01,867 - root - INFO - max_pod != pod_num, skipping closed loop
2023-11-28 20:22:01,867 - root - INFO - current max_pod: 2
2023-11-28 20:22:04,020 - root - INFO - saving CPU 0.07681022475 and max_pod 2
2023-11-28 20:22:07,192 - root - INFO - current CPU: 0.07681022475
2023-11-28 20:22:07,303 - root - INFO - max_pod != pod_num, skipping closed loop
2023-11-28 20:22:07,303 - root - INFO - current max_pod: 2
2023-11-28 20:22:09,025 - root - INFO - saving CPU 0.07681022475 and max_pod 2
2023-11-28 20:22:09,297 - root - INFO - scheduling job stress-ng --io 1 --vm 1 --vm-bytes 4G --timeout 154s
2023-11-28 20:22:09,342 - root - INFO - job scheduled
2023-11-28 20:22:12,639 - root - INFO - current CPU: 0.0889253404375
2023-11-28 20:22:12,727 - root - INFO - last jobst started 3.384651s ago, skipping closed loop
2023-11-28 20:22:12,727 - root - INFO - current max_pod: 2
2023-11-28 20:22:14,028 - root - INFO - saving CPU 0.0889253404375 and max_pod 2
2023-11-28 20:22:18,129 - root - INFO - current CPU: 0.0889253404375
2023-11-28 20:22:18,226 - root - INFO - last jobst started 8.883897s ago, skipping closed loop
2023-11-28 20:22:18,226 - root - INFO - current max_pod: 2
2023-11-28 20:22:19,032 - root - INFO - saving CPU 0.0889253404375 and max_pod 2
2023-11-28 20:22:23,576 - root - INFO - current CPU: 0.0889253404375
2023-11-28 20:22:23,700 - root - INFO - setting max_pod: 4
2023-11-28 20:22:24,036 - root - INFO - saving CPU 0.0889253404375 and max_pod 4
2023-11-28 20:22:24,438 - root - INFO - scheduling job stress-ng --io 2 --vm 1 --vm-bytes 4G --timeout 167s
2023-11-28 20:22:24,491 - root - INFO - job scheduled
2023-11-28 20:22:29,040 - root - INFO - saving CPU 0.0889253404375 and max_pod 4
2023-11-28 20:22:29,048 - root - INFO - current CPU: 0.12976226
2023-11-28 20:22:29,135 - root - INFO - max_pod != pod_num, skipping closed loop
2023-11-28 20:22:29,135 - root - INFO - current max_pod: 4
2023-11-28 20:22:34,044 - root - INFO - saving CPU 0.12976226 and max_pod 4
2023-11-28 20:22:34,515 - root - INFO - current CPU: 0.12976226
2023-11-28 20:22:34,607 - root - INFO - max_pod != pod_num, skipping closed loop
2023-11-28 20:22:34,607 - root - INFO - current max_pod: 4
2023-11-28 20:22:39,047 - root - INFO - saving CPU 0.12976226 and max_pod 4
2023-11-28 20:22:39,609 - root - INFO - scheduling job stress-ng --vm 3 --vm-bytes 2G --timeout 169s
2023-11-28 20:22:39,680 - root - INFO - job scheduled
2023-11-28 20:22:39,982 - root - INFO - current CPU: 0.12976226
2023-11-28 20:22:40,127 - root - INFO - last jobst started 0.44669s ago, skipping closed loop
2023-11-28 20:22:40,127 - root - INFO - current max_pod: 4
2023-11-28 20:22:44,052 - root - INFO - saving CPU 0.12976226 and max_pod 4
2023-11-28 20:22:45,477 - root - INFO - current CPU: 0.2052835435
2023-11-28 20:22:45,636 - root - INFO - last jobst started 5.955893s ago, skipping closed loop
2023-11-28 20:22:45,637 - root - INFO - current max_pod: 4
2023-11-28 20:22:49,056 - root - INFO - saving CPU 0.2052835435 and max_pod 4
2023-11-28 20:22:50,995 - root - INFO - current CPU: 0.2052835435
2023-11-28 20:22:51,129 - root - INFO - setting max_pod: 6
2023-11-28 20:22:54,060 - root - INFO - saving CPU 0.2052835435 and max_pod 6
2023-11-28 20:22:54,823 - root - INFO - scheduling job stress-ng --io 3 --vm 2 --vm-bytes 1G --timeout 176s
2023-11-28 20:22:54,871 - root - INFO - job scheduled
2023-11-28 20:22:56,542 - root - INFO - current CPU: 0.2052835435
2023-11-28 20:22:56,704 - root - INFO - max_pod != pod_num, skipping closed loop
2023-11-28 20:22:56,705 - root - INFO - current max_pod: 6
2023-11-28 20:22:59,064 - root - INFO - saving CPU 0.2052835435 and max_pod 6
2023-11-28 20:23:02,144 - root - INFO - current CPU: 0.3166252485
2023-11-28 20:23:02,264 - root - INFO - max_pod != pod_num, skipping closed loop
2023-11-28 20:23:02,265 - root - INFO - current max_pod: 6
2023-11-28 20:23:04,068 - root - INFO - saving CPU 0.3166252485 and max_pod 6
2023-11-28 20:23:07,681 - root - INFO - current CPU: 0.3166252485
2023-11-28 20:23:07,800 - root - INFO - max_pod != pod_num, skipping closed loop
2023-11-28 20:23:07,800 - root - INFO - current max_pod: 6
2023-11-28 20:23:09,101 - root - INFO - saving CPU 0.3166252485 and max_pod 6
2023-11-28 20:23:10,003 - root - INFO - scheduling job stress-ng --vm 3 --vm-bytes 3G --timeout 156s
2023-11-28 20:23:10,064 - root - INFO - job scheduled
2023-11-28 20:23:13,256 - root - INFO - current CPU: 0.47733379775
2023-11-28 20:23:13,428 - root - INFO - last jobst started 3.363242s ago, skipping closed loop
2023-11-28 20:23:13,428 - root - INFO - current max_pod: 6
2023-11-28 20:23:14,107 - root - INFO - saving CPU 0.47733379775 and max_pod 6
2023-11-28 20:23:18,866 - root - INFO - current CPU: 0.47733379775
2023-11-28 20:23:19,020 - root - INFO - last jobst started 8.955271s ago, skipping closed loop
2023-11-28 20:23:19,020 - root - INFO - current max_pod: 6
2023-11-28 20:23:19,112 - root - INFO - saving CPU 0.47733379775 and max_pod 6
2023-11-28 20:23:24,116 - root - INFO - saving CPU 0.47733379775 and max_pod 6
2023-11-28 20:23:24,504 - root - INFO - current CPU: 0.47733379775
2023-11-28 20:23:24,678 - root - INFO - setting max_pod: 8
2023-11-28 20:23:25,308 - root - INFO - scheduling job stress-ng --io 2 --vm 3 --vm-bytes 4G --timeout 157s
2023-11-28 20:23:25,386 - root - INFO - job scheduled
2023-11-28 20:23:29,120 - root - INFO - saving CPU 0.47733379775 and max_pod 8
2023-11-28 20:23:30,175 - root - INFO - current CPU: 0.62058428975
2023-11-28 20:23:30,352 - root - INFO - max_pod != pod_num, skipping closed loop
2023-11-28 20:23:30,353 - root - INFO - current max_pod: 8
2023-11-28 20:23:34,124 - root - INFO - saving CPU 0.62058428975 and max_pod 8
2023-11-28 20:23:35,837 - root - INFO - current CPU: 0.62058428975
2023-11-28 20:23:35,972 - root - INFO - max_pod != pod_num, skipping closed loop
2023-11-28 20:23:35,972 - root - INFO - current max_pod: 8
2023-11-28 20:23:39,128 - root - INFO - saving CPU 0.62058428975 and max_pod 8
2023-11-28 20:23:40,580 - root - INFO - scheduling job stress-ng --io 2 --vm 2 --vm-bytes 1G --timeout 176s
2023-11-28 20:23:40,653 - root - INFO - job scheduled
2023-11-28 20:23:41,585 - root - INFO - current CPU: 0.62058428975
2023-11-28 20:23:41,776 - root - INFO - last jobst started 1.122239s ago, skipping closed loop
2023-11-28 20:23:41,776 - root - INFO - current max_pod: 8
2023-11-28 20:23:44,132 - root - INFO - saving CPU 0.62058428975 and max_pod 8
2023-11-28 20:23:47,795 - root - INFO - current CPU: 0.8233938131875
2023-11-28 20:23:48,081 - root - INFO - last jobst started 7.427776s ago, skipping closed loop
2023-11-28 20:23:48,082 - root - INFO - current max_pod: 8
2023-11-28 20:23:49,136 - root - INFO - saving CPU 0.8233938131875 and max_pod 8
2023-11-28 20:23:53,958 - root - INFO - current CPU: 0.8233938131875
2023-11-28 20:23:54,140 - root - INFO - saving CPU 0.8233938131875 and max_pod 8
2023-11-28 20:23:54,175 - root - INFO - setting max_pod: 9
2023-11-28 20:23:55,873 - root - INFO - scheduling job stress-ng --vm 1 --vm-bytes 3G --timeout 164s
2023-11-28 20:23:55,937 - root - INFO - job scheduled
2023-11-28 20:23:59,144 - root - INFO - saving CPU 0.8233938131875 and max_pod 9
2023-11-28 20:24:00,126 - root - INFO - current CPU: 0.9540897359375
2023-11-28 20:24:00,394 - root - INFO - last jobst started 4.456921s ago, skipping closed loop
2023-11-28 20:24:00,394 - root - INFO - current max_pod: 9
2023-11-28 20:24:04,148 - root - INFO - saving CPU 0.9540897359375 and max_pod 9
2023-11-28 20:24:06,239 - root - INFO - current CPU: 0.9540897359375
2023-11-28 20:24:06,462 - root - INFO - setting max_pod: 9
2023-11-28 20:24:09,154 - root - INFO - saving CPU 0.9540897359375 and max_pod 9
2023-11-28 20:24:11,149 - root - INFO - current pod num: 9, max pod num: 9, job not scheduled
2023-11-28 20:24:12,354 - root - INFO - current CPU: 0.9540897359375
2023-11-28 20:24:12,569 - root - INFO - setting max_pod: 9
2023-11-28 20:24:14,156 - root - INFO - saving CPU 0.9540897359375 and max_pod 9
2023-11-28 20:24:18,516 - root - INFO - current CPU: 0.9992056399374999
2023-11-28 20:24:18,721 - root - INFO - setting max_pod: 9
2023-11-28 20:24:19,160 - root - INFO - saving CPU 0.9992056399374999 and max_pod 9
2023-11-28 20:24:24,168 - root - INFO - saving CPU 0.9992056399374999 and max_pod 9
2023-11-28 20:24:24,723 - root - INFO - current CPU: 0.9992056399374999
2023-11-28 20:24:25,025 - root - INFO - max_pod != pod_num, skipping closed loop
2023-11-28 20:24:25,025 - root - INFO - current max_pod: 9
2023-11-28 20:24:26,413 - root - INFO - scheduling job stress-ng --io 3 --vm 2 --vm-bytes 4G --timeout 155s
2023-11-28 20:24:26,511 - root - INFO - job scheduled
2023-11-28 20:24:29,172 - root - INFO - saving CPU 0.9992056399374999 and max_pod 9
2023-11-28 20:24:30,916 - root - INFO - current CPU: 0.9998012421875001
2023-11-28 20:24:31,219 - root - INFO - last jobst started 4.708259s ago, skipping closed loop
2023-11-28 20:24:31,219 - root - INFO - current max_pod: 9
2023-11-28 20:24:34,176 - root - INFO - saving CPU 0.9998012421875001 and max_pod 9
2023-11-28 20:24:37,138 - root - INFO - current CPU: 0.9998012421875001
2023-11-28 20:24:37,322 - root - INFO - setting max_pod: 8
2023-11-28 20:24:39,180 - root - INFO - saving CPU 0.9998012421875001 and max_pod 8
2023-11-28 20:24:41,721 - root - INFO - current pod num: 9, max pod num: 8, job not scheduled
2023-11-28 20:24:43,210 - root - INFO - current CPU: 0.9995032783125
2023-11-28 20:24:43,446 - root - INFO - max_pod != pod_num, skipping closed loop
2023-11-28 20:24:43,446 - root - INFO - current max_pod: 8
2023-11-28 20:24:44,184 - root - INFO - saving CPU 0.9995032783125 and max_pod 8
2023-11-28 20:24:49,192 - root - INFO - saving CPU 0.9995032783125 and max_pod 8
2023-11-28 20:24:49,310 - root - INFO - current CPU: 0.9995032783125
2023-11-28 20:24:49,633 - root - INFO - setting max_pod: 7
2023-11-28 20:24:54,196 - root - INFO - saving CPU 0.9995032783125 and max_pod 7
2023-11-28 20:24:55,552 - root - INFO - current CPU: 0.9995032783125
2023-11-28 20:24:55,805 - root - INFO - max_pod != pod_num, skipping closed loop
2023-11-28 20:24:55,806 - root - INFO - current max_pod: 7
2023-11-28 20:24:56,924 - root - INFO - current pod num: 8, max pod num: 7, job not scheduled
2023-11-28 20:24:59,200 - root - INFO - saving CPU 0.9995032783125 and max_pod 7
2023-11-28 20:25:01,692 - root - INFO - current CPU: 0.9997890085625
2023-11-28 20:25:01,970 - root - INFO - max_pod != pod_num, skipping closed loop
2023-11-28 20:25:01,970 - root - INFO - current max_pod: 7
2023-11-28 20:25:04,204 - root - INFO - saving CPU 0.9997890085625 and max_pod 7
2023-11-28 20:25:07,826 - root - INFO - current CPU: 0.9997890085625
2023-11-28 20:25:08,018 - root - INFO - max_pod != pod_num, skipping closed loop
2023-11-28 20:25:08,018 - root - INFO - current max_pod: 7
2023-11-28 20:25:09,208 - root - INFO - saving CPU 0.9997890085625 and max_pod 7
2023-11-28 20:25:12,172 - root - INFO - current pod num: 8, max pod num: 7, job not scheduled
2023-11-28 20:25:13,809 - root - INFO - current CPU: 0.999652363875
2023-11-28 20:25:14,010 - root - INFO - max_pod != pod_num, skipping closed loop
2023-11-28 20:25:14,011 - root - INFO - current max_pod: 7
2023-11-28 20:25:14,212 - root - INFO - saving CPU 0.999652363875 and max_pod 7
2023-11-28 20:25:19,216 - root - INFO - saving CPU 0.999652363875 and max_pod 7
2023-11-28 20:25:20,208 - root - INFO - current CPU: 0.999652363875
2023-11-28 20:25:20,475 - root - INFO - setting max_pod: 6
2023-11-28 20:25:24,221 - root - INFO - saving CPU 0.999652363875 and max_pod 6
2023-11-28 20:25:26,377 - root - INFO - current CPU: 0.999652363875
2023-11-28 20:25:26,634 - root - INFO - max_pod != pod_num, skipping closed loop
2023-11-28 20:25:26,635 - root - INFO - current max_pod: 6
2023-11-28 20:25:27,353 - root - INFO - current pod num: 7, max pod num: 6, job not scheduled
2023-11-28 20:25:29,224 - root - INFO - saving CPU 0.999652363875 and max_pod 6
2023-11-28 20:25:32,125 - root - INFO - current CPU: 0.9996530186875
2023-11-28 20:25:32,325 - root - INFO - max_pod != pod_num, skipping closed loop
2023-11-28 20:25:32,326 - root - INFO - current max_pod: 6
2023-11-28 20:25:34,228 - root - INFO - saving CPU 0.9996530186875 and max_pod 6
2023-11-28 20:25:37,847 - root - INFO - current CPU: 0.9996530186875
2023-11-28 20:25:38,068 - root - INFO - setting max_pod: 5
2023-11-28 20:25:39,232 - root - INFO - saving CPU 0.9996530186875 and max_pod 5
2023-11-28 20:25:42,516 - root - INFO - current pod num: 6, max pod num: 5, job not scheduled
2023-11-28 20:25:43,584 - root - INFO - current CPU: 0.9731158772500001
2023-11-28 20:25:43,752 - root - INFO - max_pod != pod_num, skipping closed loop
2023-11-28 20:25:43,752 - root - INFO - current max_pod: 5
2023-11-28 20:25:44,236 - root - INFO - saving CPU 0.9731158772500001 and max_pod 5
2023-11-28 20:25:49,159 - root - INFO - current CPU: 0.9731158772500001
2023-11-28 20:25:49,240 - root - INFO - saving CPU 0.9731158772500001 and max_pod 5
2023-11-28 20:25:49,288 - root - INFO - max_pod != pod_num, skipping closed loop
2023-11-28 20:25:49,288 - root - INFO - current max_pod: 5
2023-11-28 20:25:54,245 - root - INFO - saving CPU 0.9731158772500001 and max_pod 5
2023-11-28 20:25:54,677 - root - INFO - current CPU: 0.9731158772500001
2023-11-28 20:25:54,910 - root - INFO - setting max_pod: 4
2023-11-28 20:25:57,838 - root - INFO - current pod num: 4, max pod num: 4, job not scheduled
2023-11-28 20:25:59,252 - root - INFO - saving CPU 0.9731158772500001 and max_pod 4
2023-11-28 20:26:00,361 - root - INFO - current CPU: 0.7971608989375
2023-11-28 20:26:00,505 - root - INFO - setting max_pod: 3
2023-11-28 20:26:04,256 - root - INFO - saving CPU 0.7971608989375 and max_pod 3
2023-11-28 20:26:05,961 - root - INFO - current CPU: 0.7971608989375
2023-11-28 20:26:06,143 - root - INFO - max_pod != pod_num, skipping closed loop
2023-11-28 20:26:06,143 - root - INFO - current max_pod: 3
2023-11-28 20:26:09,260 - root - INFO - saving CPU 0.7971608989375 and max_pod 3
2023-11-28 20:26:11,480 - root - INFO - current CPU: 0.7971608989375
2023-11-28 20:26:11,651 - root - INFO - setting max_pod: 3
2023-11-28 20:26:12,945 - root - INFO - current pod num: 3, max pod num: 3, job not scheduled
2023-11-28 20:26:14,264 - root - INFO - saving CPU 0.7971608989375 and max_pod 3
2023-11-28 20:26:17,027 - root - INFO - current CPU: 0.52965764325
2023-11-28 20:26:17,196 - root - INFO - setting max_pod: 3
2023-11-28 20:26:19,268 - root - INFO - saving CPU 0.52965764325 and max_pod 3
2023-11-28 20:26:22,686 - root - INFO - current CPU: 0.52965764325
2023-11-28 20:26:22,792 - root - INFO - setting max_pod: 4
2023-11-28 20:26:24,272 - root - INFO - saving CPU 0.52965764325 and max_pod 4
2023-11-28 20:26:28,057 - root - INFO - scheduling job stress-ng --io 1 --vm 1 --vm-bytes 3G --timeout 172s
2023-11-28 20:26:28,106 - root - INFO - job scheduled
2023-11-28 20:26:28,234 - root - INFO - current CPU: 0.3524282230625
2023-11-28 20:26:28,405 - root - INFO - last jobst started 0.298424s ago, skipping closed loop
2023-11-28 20:26:28,405 - root - INFO - current max_pod: 4
2023-11-28 20:26:29,276 - root - INFO - saving CPU 0.3524282230625 and max_pod 4
2023-11-28 20:26:33,789 - root - INFO - current CPU: 0.3524282230625
2023-11-28 20:26:33,892 - root - INFO - last jobst started 5.785771s ago, skipping closed loop
2023-11-28 20:26:33,892 - root - INFO - current max_pod: 4
2023-11-28 20:26:34,280 - root - INFO - saving CPU 0.3524282230625 and max_pod 4
2023-11-28 20:26:39,218 - root - INFO - current CPU: 0.3524282230625
2023-11-28 20:26:39,284 - root - INFO - saving CPU 0.3524282230625 and max_pod 4
2023-11-28 20:26:39,371 - root - INFO - setting max_pod: 5
2023-11-28 20:26:43,245 - root - INFO - scheduling job stress-ng --io 3 --vm 3 --vm-bytes 4G --timeout 150s
2023-11-28 20:26:43,293 - root - INFO - job scheduled
2023-11-28 20:26:44,288 - root - INFO - saving CPU 0.3524282230625 and max_pod 5
2023-11-28 20:26:44,704 - root - INFO - current CPU: 0.3719014435
2023-11-28 20:26:44,866 - root - INFO - max_pod != pod_num, skipping closed loop
2023-11-28 20:26:44,866 - root - INFO - current max_pod: 5
2023-11-28 20:26:49,292 - root - INFO - saving CPU 0.3719014435 and max_pod 5
2023-11-28 20:26:50,211 - root - INFO - current CPU: 0.3719014435
2023-11-28 20:26:50,395 - root - INFO - max_pod != pod_num, skipping closed loop
2023-11-28 20:26:50,395 - root - INFO - current max_pod: 5
2023-11-28 20:26:54,296 - root - INFO - saving CPU 0.3719014435 and max_pod 5
2023-11-28 20:26:55,879 - root - INFO - current CPU: 0.3719014435
2023-11-28 20:26:55,971 - root - INFO - max_pod != pod_num, skipping closed loop
2023-11-28 20:26:55,972 - root - INFO - current max_pod: 5
2023-11-28 20:26:58,412 - root - INFO - scheduling job stress-ng --io 2 --vm 1 --vm-bytes 3G --timeout 155s
2023-11-28 20:26:58,481 - root - INFO - job scheduled
2023-11-28 20:26:59,302 - root - INFO - saving CPU 0.3719014435 and max_pod 5
2023-11-28 20:27:01,407 - root - INFO - current CPU: 0.3654219325
2023-11-28 20:27:01,580 - root - INFO - max_pod != pod_num, skipping closed loop
2023-11-28 20:27:01,580 - root - INFO - current max_pod: 5
2023-11-28 20:27:04,304 - root - INFO - saving CPU 0.3654219325 and max_pod 5
2023-11-28 20:27:07,009 - root - INFO - current CPU: 0.3654219325
2023-11-28 20:27:07,207 - root - INFO - max_pod != pod_num, skipping closed loop
2023-11-28 20:27:07,208 - root - INFO - current max_pod: 5
2023-11-28 20:27:09,309 - root - INFO - saving CPU 0.3654219325 and max_pod 5
2023-11-28 20:27:12,662 - root - INFO - current CPU: 0.4366732968125
2023-11-28 20:27:12,842 - root - INFO - max_pod != pod_num, skipping closed loop
2023-11-28 20:27:12,843 - root - INFO - current max_pod: 5
2023-11-28 20:27:13,583 - root - INFO - scheduling job stress-ng --io 1 --vm 2 --vm-bytes 2G --timeout 160s
2023-11-28 20:27:13,633 - root - INFO - job scheduled
2023-11-28 20:27:14,316 - root - INFO - saving CPU 0.4366732968125 and max_pod 5
2023-11-28 20:27:18,317 - root - INFO - current CPU: 0.4366732968125
2023-11-28 20:27:18,451 - root - INFO - max_pod != pod_num, skipping closed loop
2023-11-28 20:27:18,451 - root - INFO - current max_pod: 5
2023-11-28 20:27:19,320 - root - INFO - saving CPU 0.4366732968125 and max_pod 5
2023-11-28 20:27:23,851 - root - INFO - current CPU: 0.4366732968125
2023-11-28 20:27:24,001 - root - INFO - max_pod != pod_num, skipping closed loop
2023-11-28 20:27:24,002 - root - INFO - current max_pod: 5
2023-11-28 20:27:24,328 - root - INFO - saving CPU 0.4366732968125 and max_pod 5
2023-11-28 20:27:28,743 - root - INFO - scheduling job stress-ng --io 1 --vm 3 --vm-bytes 1G --timeout 168s
2023-11-28 20:27:28,796 - root - INFO - job scheduled
2023-11-28 20:27:29,332 - root - INFO - saving CPU 0.4366732968125 and max_pod 5
2023-11-28 20:27:29,360 - root - INFO - current CPU: 0.40036826912500006
2023-11-28 20:27:29,491 - root - INFO - last jobst started 0.695413s ago, skipping closed loop
2023-11-28 20:27:29,495 - root - INFO - current max_pod: 5
2023-11-28 20:27:34,336 - root - INFO - saving CPU 0.40036826912500006 and max_pod 5
2023-11-28 20:27:34,953 - root - INFO - current CPU: 0.40036826912500006
2023-11-28 20:27:35,104 - root - INFO - last jobst started 6.308628s ago, skipping closed loop
2023-11-28 20:27:35,105 - root - INFO - current max_pod: 5
2023-11-28 20:27:39,340 - root - INFO - saving CPU 0.40036826912500006 and max_pod 5
2023-11-28 20:27:40,498 - root - INFO - current CPU: 0.40036826912500006
2023-11-28 20:27:40,625 - root - INFO - setting max_pod: 6
2023-11-28 20:27:43,930 - root - INFO - scheduling job stress-ng --io 1 --vm 2 --vm-bytes 4G --timeout 175s
2023-11-28 20:27:44,044 - root - INFO - job scheduled
2023-11-28 20:27:44,344 - root - INFO - saving CPU 0.40036826912500006 and max_pod 6
2023-11-28 20:27:46,105 - root - INFO - current CPU: 0.526653773625
2023-11-28 20:27:46,289 - root - INFO - last jobst started 2.245195s ago, skipping closed loop
2023-11-28 20:27:46,289 - root - INFO - current max_pod: 6
2023-11-28 20:27:49,360 - root - INFO - saving CPU 0.526653773625 and max_pod 6
2023-11-28 20:27:51,726 - root - INFO - current CPU: 0.526653773625
2023-11-28 20:27:51,885 - root - INFO - last jobst started 7.841388s ago, skipping closed loop
2023-11-28 20:27:51,886 - root - INFO - current max_pod: 6
2023-11-28 20:27:54,364 - root - INFO - saving CPU 0.526653773625 and max_pod 6
2023-11-28 20:27:57,347 - root - INFO - current CPU: 0.7033495796875
2023-11-28 20:27:57,505 - root - INFO - setting max_pod: 7
2023-11-28 20:27:59,195 - root - INFO - scheduling job stress-ng --io 1 --timeout 172s
2023-11-28 20:27:59,254 - root - INFO - job scheduled
2023-11-28 20:27:59,368 - root - INFO - saving CPU 0.7033495796875 and max_pod 7
2023-11-28 20:28:02,971 - root - INFO - current CPU: 0.7033495796875
2023-11-28 20:28:03,155 - root - INFO - last jobst started 3.900688s ago, skipping closed loop
2023-11-28 20:28:03,155 - root - INFO - current max_pod: 7
2023-11-28 20:28:04,372 - root - INFO - saving CPU 0.7033495796875 and max_pod 7
2023-11-28 20:28:08,593 - root - INFO - current CPU: 0.7033495796875
2023-11-28 20:28:08,750 - root - INFO - last jobst started 9.495992s ago, skipping closed loop
2023-11-28 20:28:08,750 - root - INFO - current max_pod: 7
2023-11-28 20:28:09,377 - root - INFO - saving CPU 0.7033495796875 and max_pod 7
2023-11-28 20:28:14,206 - root - INFO - current CPU: 0.7861613351875001
2023-11-28 20:28:14,381 - root - INFO - saving CPU 0.7861613351875001 and max_pod 7
2023-11-28 20:28:14,477 - root - INFO - setting max_pod: 7
2023-11-28 20:28:14,579 - root - INFO - current pod num: 7, max pod num: 7, job not scheduled
2023-11-28 20:28:19,384 - root - INFO - saving CPU 0.7861613351875001 and max_pod 7
2023-11-28 20:28:19,918 - root - INFO - current CPU: 0.7861613351875001
2023-11-28 20:28:20,047 - root - INFO - setting max_pod: 7
2023-11-28 20:28:24,388 - root - INFO - saving CPU 0.7861613351875001 and max_pod 7
2023-11-28 20:28:25,491 - root - INFO - current CPU: 0.7861613351875001
2023-11-28 20:28:25,669 - root - INFO - setting max_pod: 7
2023-11-28 20:28:29,396 - root - INFO - saving CPU 0.7861613351875001 and max_pod 7
2023-11-28 20:28:29,747 - root - INFO - current pod num: 7, max pod num: 7, job not scheduled
2023-11-28 20:28:31,078 - root - INFO - current CPU: 0.77754731975
2023-11-28 20:28:31,254 - root - INFO - setting max_pod: 7
2023-11-28 20:28:34,400 - root - INFO - saving CPU 0.77754731975 and max_pod 7
2023-11-28 20:28:36,715 - root - INFO - current CPU: 0.77754731975
2023-11-28 20:28:36,881 - root - INFO - setting max_pod: 7
2023-11-28 20:28:39,404 - root - INFO - saving CPU 0.77754731975 and max_pod 7
2023-11-28 20:28:42,355 - root - INFO - current CPU: 0.77929047
2023-11-28 20:28:42,528 - root - INFO - setting max_pod: 7
2023-11-28 20:28:44,408 - root - INFO - saving CPU 0.77929047 and max_pod 7
2023-11-28 20:28:44,991 - root - INFO - current pod num: 7, max pod num: 7, job not scheduled
2023-11-28 20:28:48,023 - root - INFO - current CPU: 0.77929047
2023-11-28 20:28:48,203 - root - INFO - setting max_pod: 7
2023-11-28 20:28:49,412 - root - INFO - saving CPU 0.77929047 and max_pod 7
2023-11-28 20:28:53,638 - root - INFO - current CPU: 0.77929047
2023-11-28 20:28:53,789 - root - INFO - setting max_pod: 7
2023-11-28 20:28:54,416 - root - INFO - saving CPU 0.77929047 and max_pod 7
2023-11-28 20:28:59,232 - root - INFO - current CPU: 0.7791538308125
2023-11-28 20:28:59,414 - root - INFO - setting max_pod: 7
2023-11-28 20:28:59,420 - root - INFO - saving CPU 0.7791538308125 and max_pod 7
2023-11-28 20:29:00,178 - root - INFO - current pod num: 7, max pod num: 7, job not scheduled
2023-11-28 20:29:04,424 - root - INFO - saving CPU 0.7791538308125 and max_pod 7
2023-11-28 20:29:04,881 - root - INFO - current CPU: 0.7791538308125
2023-11-28 20:29:05,094 - root - INFO - setting max_pod: 7
2023-11-28 20:29:09,428 - root - INFO - saving CPU 0.7791538308125 and max_pod 7
2023-11-28 20:29:10,593 - root - INFO - current CPU: 0.7791538308125
2023-11-28 20:29:10,766 - root - INFO - setting max_pod: 7
2023-11-28 20:29:14,432 - root - INFO - saving CPU 0.7791538308125 and max_pod 7
2023-11-28 20:29:15,377 - root - INFO - current pod num: 7, max pod num: 7, job not scheduled
2023-11-28 20:29:16,342 - root - INFO - current CPU: 0.7776591518124999
2023-11-28 20:29:16,503 - root - INFO - setting max_pod: 7
2023-11-28 20:29:19,436 - root - INFO - saving CPU 0.7776591518124999 and max_pod 7
2023-11-28 20:29:21,894 - root - INFO - current CPU: 0.7776591518124999
2023-11-28 20:29:22,111 - root - INFO - max_pod != pod_num, skipping closed loop
2023-11-28 20:29:22,111 - root - INFO - current max_pod: 7
2023-11-28 20:29:24,440 - root - INFO - saving CPU 0.7776591518124999 and max_pod 7
2023-11-28 20:29:27,537 - root - INFO - current CPU: 0.6897991648125
2023-11-28 20:29:27,722 - root - INFO - max_pod != pod_num, skipping closed loop
2023-11-28 20:29:27,722 - root - INFO - current max_pod: 7
2023-11-28 20:29:29,444 - root - INFO - saving CPU 0.6897991648125 and max_pod 7
2023-11-28 20:29:30,531 - root - INFO - scheduling job stress-ng --io 1 --vm 2 --vm-bytes 2G --timeout 151s
2023-11-28 20:29:30,604 - root - INFO - job scheduled
2023-11-28 20:29:33,252 - root - INFO - current CPU: 0.6897991648125
2023-11-28 20:29:33,406 - root - INFO - max_pod != pod_num, skipping closed loop
2023-11-28 20:29:33,406 - root - INFO - current max_pod: 7
2023-11-28 20:29:34,452 - root - INFO - saving CPU 0.6897991648125 and max_pod 7
2023-11-28 20:29:38,922 - root - INFO - current CPU: 0.6897991648125
2023-11-28 20:29:39,121 - root - INFO - max_pod != pod_num, skipping closed loop
2023-11-28 20:29:39,121 - root - INFO - current max_pod: 7
2023-11-28 20:29:39,456 - root - INFO - saving CPU 0.6897991648125 and max_pod 7
2023-11-28 20:29:44,460 - root - INFO - saving CPU 0.6897991648125 and max_pod 7
2023-11-28 20:29:44,563 - root - INFO - current CPU: 0.568450900375
2023-11-28 20:29:44,746 - root - INFO - max_pod != pod_num, skipping closed loop
2023-11-28 20:29:44,746 - root - INFO - current max_pod: 7
2023-11-28 20:29:45,765 - root - INFO - scheduling job stress-ng --io 1 --vm 3 --vm-bytes 4G --timeout 179s
2023-11-28 20:29:45,831 - root - INFO - job scheduled
2023-11-28 20:29:49,464 - root - INFO - saving CPU 0.568450900375 and max_pod 7
2023-11-28 20:29:50,212 - root - INFO - current CPU: 0.568450900375
2023-11-28 20:29:50,364 - root - INFO - max_pod != pod_num, skipping closed loop
2023-11-28 20:29:50,364 - root - INFO - current max_pod: 7
2023-11-28 20:29:54,470 - root - INFO - saving CPU 0.568450900375 and max_pod 7
2023-11-28 20:29:55,903 - root - INFO - current CPU: 0.568450900375
2023-11-28 20:29:56,113 - root - INFO - max_pod != pod_num, skipping closed loop
2023-11-28 20:29:56,114 - root - INFO - current max_pod: 7
2023-11-28 20:29:59,476 - root - INFO - saving CPU 0.568450900375 and max_pod 7
2023-11-28 20:30:01,030 - root - INFO - scheduling job stress-ng --io 2 --vm 2 --vm-bytes 4G --timeout 180s
2023-11-28 20:30:01,171 - root - INFO - job scheduled
2023-11-28 20:30:01,583 - root - INFO - current CPU: 0.668133233875
2023-11-28 20:30:01,737 - root - INFO - max_pod != pod_num, skipping closed loop
2023-11-28 20:30:01,737 - root - INFO - current max_pod: 7
2023-11-28 20:30:04,480 - root - INFO - saving CPU 0.668133233875 and max_pod 7
2023-11-28 20:30:07,177 - root - INFO - current CPU: 0.668133233875
2023-11-28 20:30:07,332 - root - INFO - max_pod != pod_num, skipping closed loop
2023-11-28 20:30:07,332 - root - INFO - current max_pod: 7
2023-11-28 20:30:09,484 - root - INFO - saving CPU 0.668133233875 and max_pod 7
2023-11-28 20:30:12,815 - root - INFO - current CPU: 0.6958258883749999
2023-11-28 20:30:12,976 - root - INFO - max_pod != pod_num, skipping closed loop
2023-11-28 20:30:12,976 - root - INFO - current max_pod: 7
2023-11-28 20:30:14,488 - root - INFO - saving CPU 0.6958258883749999 and max_pod 7
2023-11-28 20:30:16,406 - root - INFO - scheduling job stress-ng --io 1 --vm 3 --vm-bytes 3G --timeout 164s
2023-11-28 20:30:16,473 - root - INFO - job scheduled
2023-11-28 20:30:18,658 - root - INFO - current CPU: 0.6958258883749999
2023-11-28 20:30:18,833 - root - INFO - last jobst started 2.35995s ago, skipping closed loop
2023-11-28 20:30:18,834 - root - INFO - current max_pod: 7
2023-11-28 20:30:19,492 - root - INFO - saving CPU 0.6958258883749999 and max_pod 7
2023-11-28 20:30:24,255 - root - INFO - current CPU: 0.6958258883749999
2023-11-28 20:30:24,456 - root - INFO - max_pod != pod_num, skipping closed loop
2023-11-28 20:30:24,456 - root - INFO - current max_pod: 7
2023-11-28 20:30:24,496 - root - INFO - saving CPU 0.6958258883749999 and max_pod 7
2023-11-28 20:30:29,504 - root - INFO - saving CPU 0.6958258883749999 and max_pod 7
2023-11-28 20:30:29,906 - root - INFO - current CPU: 0.7911216115
2023-11-28 20:30:30,017 - root - INFO - max_pod != pod_num, skipping closed loop
2023-11-28 20:30:30,018 - root - INFO - current max_pod: 7
2023-11-28 20:30:31,645 - root - INFO - scheduling job stress-ng --io 1 --vm 3 --vm-bytes 3G --timeout 155s
2023-11-28 20:30:31,711 - root - INFO - job scheduled
2023-11-28 20:30:34,508 - root - INFO - saving CPU 0.7911216115 and max_pod 7
2023-11-28 20:30:35,656 - root - INFO - current CPU: 0.7911216115
2023-11-28 20:30:35,854 - root - INFO - last jobst started 4.143117s ago, skipping closed loop
2023-11-28 20:30:35,854 - root - INFO - current max_pod: 7
2023-11-28 20:30:39,512 - root - INFO - saving CPU 0.7911216115 and max_pod 7
2023-11-28 20:30:41,423 - root - INFO - current CPU: 0.7911216115
2023-11-28 20:30:41,603 - root - INFO - last jobst started 9.891973s ago, skipping closed loop
2023-11-28 20:30:41,603 - root - INFO - current max_pod: 7
2023-11-28 20:30:44,516 - root - INFO - saving CPU 0.7911216115 and max_pod 7
2023-11-28 20:30:46,951 - root - INFO - scheduling job stress-ng --io 3 --timeout 151s
2023-11-28 20:30:47,023 - root - INFO - job scheduled
2023-11-28 20:30:47,413 - root - INFO - current CPU: 0.8178993935625001
2023-11-28 20:30:47,592 - root - INFO - last jobst started 0.569502s ago, skipping closed loop
2023-11-28 20:30:47,592 - root - INFO - current max_pod: 7
2023-11-28 20:30:49,520 - root - INFO - saving CPU 0.8178993935625001 and max_pod 7
2023-11-28 20:30:53,067 - root - INFO - current CPU: 0.8178993935625001
2023-11-28 20:30:53,272 - root - INFO - last jobst started 6.249364s ago, skipping closed loop
2023-11-28 20:30:53,273 - root - INFO - current max_pod: 7
2023-11-28 20:30:54,524 - root - INFO - saving CPU 0.8178993935625001 and max_pod 7
2023-11-28 20:30:58,781 - root - INFO - current CPU: 0.8848429110625
2023-11-28 20:30:59,025 - root - INFO - max_pod != pod_num, skipping closed loop
2023-11-28 20:30:59,025 - root - INFO - current max_pod: 7
2023-11-28 20:30:59,532 - root - INFO - saving CPU 0.8848429110625 and max_pod 7
2023-11-28 20:31:02,188 - root - INFO - scheduling job stress-ng --io 2 --vm 1 --vm-bytes 2G --timeout 168s
2023-11-28 20:31:02,259 - root - INFO - job scheduled
2023-11-28 20:31:04,536 - root - INFO - saving CPU 0.8848429110625 and max_pod 7
2023-11-28 20:31:04,542 - root - INFO - current CPU: 0.8848429110625
2023-11-28 20:31:04,709 - root - INFO - last jobst started 2.450037s ago, skipping closed loop
2023-11-28 20:31:04,710 - root - INFO - current max_pod: 7
2023-11-28 20:31:09,540 - root - INFO - saving CPU 0.8848429110625 and max_pod 7
2023-11-28 20:31:10,213 - root - INFO - current CPU: 0.8848429110625
2023-11-28 20:31:10,384 - root - INFO - last jobst started 8.124828s ago, skipping closed loop
2023-11-28 20:31:10,384 - root - INFO - current max_pod: 7
2023-11-28 20:31:14,544 - root - INFO - saving CPU 0.8848429110625 and max_pod 7
2023-11-28 20:31:15,918 - root - INFO - current CPU: 0.856238204
2023-11-28 20:31:16,103 - root - INFO - setting max_pod: 7
2023-11-28 20:31:17,429 - root - INFO - current pod num: 7, max pod num: 7, job not scheduled
2023-11-28 20:31:19,548 - root - INFO - saving CPU 0.856238204 and max_pod 7
2023-11-28 20:31:21,610 - root - INFO - current CPU: 0.856238204
2023-11-28 20:31:21,776 - root - INFO - setting max_pod: 7
2023-11-28 20:31:24,552 - root - INFO - saving CPU 0.856238204 and max_pod 7
2023-11-28 20:31:27,250 - root - INFO - current CPU: 0.856238204
2023-11-28 20:31:27,427 - root - INFO - setting max_pod: 7
2023-11-28 20:31:29,556 - root - INFO - saving CPU 0.856238204 and max_pod 7
2023-11-28 20:31:32,640 - root - INFO - current pod num: 7, max pod num: 7, job not scheduled
2023-11-28 20:31:33,034 - root - INFO - current CPU: 0.9076419648124999
2023-11-28 20:31:33,216 - root - INFO - setting max_pod: 7
2023-11-28 20:31:34,560 - root - INFO - saving CPU 0.9076419648124999 and max_pod 7
2023-11-28 20:31:38,763 - root - INFO - current CPU: 0.9076419648124999
2023-11-28 20:31:38,954 - root - INFO - setting max_pod: 7
2023-11-28 20:31:39,564 - root - INFO - saving CPU 0.9076419648124999 and max_pod 7
2023-11-28 20:31:44,467 - root - INFO - current CPU: 0.9089781745625
2023-11-28 20:31:44,570 - root - INFO - saving CPU 0.9089781745625 and max_pod 7
2023-11-28 20:31:44,649 - root - INFO - setting max_pod: 7
2023-11-28 20:31:47,810 - root - INFO - current pod num: 7, max pod num: 7, job not scheduled
2023-11-28 20:31:49,576 - root - INFO - saving CPU 0.9089781745625 and max_pod 7
2023-11-28 20:31:50,208 - root - INFO - current CPU: 0.9089781745625
2023-11-28 20:31:50,401 - root - INFO - setting max_pod: 7
2023-11-28 20:31:54,580 - root - INFO - saving CPU 0.9089781745625 and max_pod 7
2023-11-28 20:31:56,001 - root - INFO - current CPU: 0.9089781745625
2023-11-28 20:31:56,166 - root - INFO - setting max_pod: 7
2023-11-28 20:31:59,584 - root - INFO - saving CPU 0.9089781745625 and max_pod 7
2023-11-28 20:32:01,646 - root - INFO - current CPU: 0.908709052
2023-11-28 20:32:01,839 - root - INFO - setting max_pod: 7
2023-11-28 20:32:03,086 - root - INFO - current pod num: 7, max pod num: 7, job not scheduled
2023-11-28 20:32:04,592 - root - INFO - saving CPU 0.908709052 and max_pod 7
2023-11-28 20:32:07,269 - root - INFO - current CPU: 0.908709052
2023-11-28 20:32:07,494 - root - INFO - max_pod != pod_num, skipping closed loop
2023-11-28 20:32:07,494 - root - INFO - current max_pod: 7
2023-11-28 20:32:09,596 - root - INFO - saving CPU 0.908709052 and max_pod 7
2023-11-28 20:32:12,978 - root - INFO - current CPU: 0.89122876725
2023-11-28 20:32:13,145 - root - INFO - max_pod != pod_num, skipping closed loop
2023-11-28 20:32:13,145 - root - INFO - current max_pod: 7
2023-11-28 20:32:14,600 - root - INFO - saving CPU 0.89122876725 and max_pod 7
2023-11-28 20:32:18,265 - root - INFO - scheduling job stress-ng --io 1 --vm 3 --vm-bytes 1G --timeout 169s
2023-11-28 20:32:18,331 - root - INFO - job scheduled
2023-11-28 20:32:18,629 - root - INFO - current CPU: 0.89122876725
2023-11-28 20:32:18,795 - root - INFO - last jobst started 0.463695s ago, skipping closed loop
2023-11-28 20:32:18,795 - root - INFO - current max_pod: 7
2023-11-28 20:32:19,604 - root - INFO - saving CPU 0.89122876725 and max_pod 7
2023-11-28 20:32:24,498 - root - INFO - current CPU: 0.89122876725
2023-11-28 20:32:24,608 - root - INFO - saving CPU 0.89122876725 and max_pod 7
2023-11-28 20:32:24,674 - root - INFO - last jobst started 6.342982s ago, skipping closed loop
2023-11-28 20:32:24,674 - root - INFO - current max_pod: 7
2023-11-28 20:32:29,612 - root - INFO - saving CPU 0.89122876725 and max_pod 7
2023-11-28 20:32:30,321 - root - INFO - current CPU: 0.833217494875
2023-11-28 20:32:30,491 - root - INFO - setting max_pod: 7
2023-11-28 20:32:33,514 - root - INFO - current pod num: 7, max pod num: 7, job not scheduled
2023-11-28 20:32:34,616 - root - INFO - saving CPU 0.833217494875 and max_pod 7
2023-11-28 20:32:36,213 - root - INFO - current CPU: 0.833217494875
2023-11-28 20:32:36,390 - root - INFO - setting max_pod: 7
2023-11-28 20:32:39,620 - root - INFO - saving CPU 0.833217494875 and max_pod 7
2023-11-28 20:32:41,987 - root - INFO - current CPU: 0.833217494875
2023-11-28 20:32:42,235 - root - INFO - setting max_pod: 7
2023-11-28 20:32:44,624 - root - INFO - saving CPU 0.833217494875 and max_pod 7
2023-11-28 20:32:47,734 - root - INFO - current CPU: 0.9623893805
2023-11-28 20:32:47,902 - root - INFO - setting max_pod: 7
2023-11-28 20:32:48,697 - root - INFO - current pod num: 7, max pod num: 7, job not scheduled
2023-11-28 20:32:49,628 - root - INFO - saving CPU 0.9623893805 and max_pod 7
2023-11-28 20:32:53,403 - root - INFO - current CPU: 0.9623893805
2023-11-28 20:32:53,617 - root - INFO - max_pod != pod_num, skipping closed loop
2023-11-28 20:32:53,617 - root - INFO - current max_pod: 7
2023-11-28 20:32:54,636 - root - INFO - saving CPU 0.9623893805 and max_pod 7
2023-11-28 20:32:59,039 - root - INFO - current CPU: 0.884951794
2023-11-28 20:32:59,213 - root - INFO - max_pod != pod_num, skipping closed loop
2023-11-28 20:32:59,214 - root - INFO - current max_pod: 7
2023-11-28 20:32:59,640 - root - INFO - saving CPU 0.884951794 and max_pod 7
2023-11-28 20:33:03,876 - root - INFO - scheduling job stress-ng --vm 2 --vm-bytes 1G --timeout 167s
2023-11-28 20:33:03,946 - root - INFO - job scheduled
2023-11-28 20:33:04,644 - root - INFO - saving CPU 0.884951794 and max_pod 7
2023-11-28 20:33:04,697 - root - INFO - current CPU: 0.884951794
2023-11-28 20:33:04,921 - root - INFO - last jobst started 0.975069s ago, skipping closed loop
2023-11-28 20:33:04,922 - root - INFO - current max_pod: 7
2023-11-28 20:33:09,648 - root - INFO - saving CPU 0.884951794 and max_pod 7
2023-11-28 20:33:10,257 - root - INFO - current CPU: 0.884951794
2023-11-28 20:33:10,528 - root - INFO - max_pod != pod_num, skipping closed loop
2023-11-28 20:33:10,529 - root - INFO - current max_pod: 7
2023-11-28 20:33:14,652 - root - INFO - saving CPU 0.884951794 and max_pod 7
2023-11-28 20:33:15,980 - root - INFO - current CPU: 0.71491053675
2023-11-28 20:33:16,216 - root - INFO - max_pod != pod_num, skipping closed loop
2023-11-28 20:33:16,217 - root - INFO - current max_pod: 7
2023-11-28 20:33:19,062 - root - INFO - scheduling job stress-ng --io 1 --vm 1 --vm-bytes 3G --timeout 153s
2023-11-28 20:33:19,115 - root - INFO - job scheduled
2023-11-28 20:33:19,656 - root - INFO - saving CPU 0.71491053675 and max_pod 7
2023-11-28 20:33:21,616 - root - INFO - current CPU: 0.71491053675
2023-11-28 20:33:21,756 - root - INFO - max_pod != pod_num, skipping closed loop
2023-11-28 20:33:21,756 - root - INFO - current max_pod: 7
2023-11-28 20:33:24,662 - root - INFO - saving CPU 0.71491053675 and max_pod 7
2023-11-28 20:33:27,226 - root - INFO - current CPU: 0.71491053675
2023-11-28 20:33:27,497 - root - INFO - max_pod != pod_num, skipping closed loop
2023-11-28 20:33:27,497 - root - INFO - current max_pod: 7
2023-11-28 20:33:29,668 - root - INFO - saving CPU 0.71491053675 and max_pod 7
2023-11-28 20:33:32,994 - root - INFO - current CPU: 0.45068162593750005
2023-11-28 20:33:33,137 - root - INFO - max_pod != pod_num, skipping closed loop
2023-11-28 20:33:33,137 - root - INFO - current max_pod: 7
2023-11-28 20:33:34,237 - root - INFO - scheduling job stress-ng --io 3 --vm 3 --vm-bytes 4G --timeout 155s
2023-11-28 20:33:34,345 - root - INFO - job scheduled
2023-11-28 20:33:34,676 - root - INFO - saving CPU 0.45068162593750005 and max_pod 7
2023-11-28 20:33:38,585 - root - INFO - current CPU: 0.45068162593750005
2023-11-28 20:33:38,838 - root - INFO - max_pod != pod_num, skipping closed loop
2023-11-28 20:33:38,838 - root - INFO - current max_pod: 7
2023-11-28 20:33:39,680 - root - INFO - saving CPU 0.45068162593750005 and max_pod 7
2023-11-28 20:33:44,405 - root - INFO - current CPU: 0.48306812525
2023-11-28 20:33:44,591 - root - INFO - max_pod != pod_num, skipping closed loop
2023-11-28 20:33:44,591 - root - INFO - current max_pod: 7
2023-11-28 20:33:44,688 - root - INFO - saving CPU 0.48306812525 and max_pod 7
2023-11-28 20:33:49,540 - root - INFO - scheduling job stress-ng --io 1 --vm 3 --vm-bytes 3G --timeout 170s
2023-11-28 20:33:49,615 - root - INFO - job scheduled
2023-11-28 20:33:49,696 - root - INFO - saving CPU 0.48306812525 and max_pod 7
2023-11-28 20:33:50,075 - root - INFO - current CPU: 0.48306812525
2023-11-28 20:33:50,249 - root - INFO - max_pod != pod_num, skipping closed loop
2023-11-28 20:33:50,249 - root - INFO - current max_pod: 7
2023-11-28 20:33:54,700 - root - INFO - saving CPU 0.48306812525 and max_pod 7
2023-11-28 20:33:55,693 - root - INFO - current CPU: 0.48306812525
2023-11-28 20:33:55,886 - root - INFO - max_pod != pod_num, skipping closed loop
2023-11-28 20:33:55,886 - root - INFO - current max_pod: 7
2023-11-28 20:33:59,704 - root - INFO - saving CPU 0.48306812525 and max_pod 7
2023-11-28 20:34:01,409 - root - INFO - current CPU: 0.6962698057499999
2023-11-28 20:34:01,587 - root - INFO - max_pod != pod_num, skipping closed loop
2023-11-28 20:34:01,587 - root - INFO - current max_pod: 7
2023-11-28 20:34:04,712 - root - INFO - saving CPU 0.6962698057499999 and max_pod 7
2023-11-28 20:34:04,798 - root - INFO - scheduling job stress-ng --io 1 --vm 3 --vm-bytes 2G --timeout 176s
2023-11-28 20:34:04,872 - root - INFO - job scheduled
2023-11-28 20:34:07,249 - root - INFO - current CPU: 0.6962698057499999
2023-11-28 20:34:07,436 - root - INFO - max_pod != pod_num, skipping closed loop
2023-11-28 20:34:07,436 - root - INFO - current max_pod: 7
2023-11-28 20:34:09,716 - root - INFO - saving CPU 0.6962698057499999 and max_pod 7
2023-11-28 20:34:13,155 - root - INFO - current CPU: 0.7860796583125
2023-11-28 20:34:13,443 - root - INFO - max_pod != pod_num, skipping closed loop
2023-11-28 20:34:13,443 - root - INFO - current max_pod: 7
2023-11-28 20:34:14,724 - root - INFO - saving CPU 0.7860796583125 and max_pod 7
2023-11-28 20:34:19,627 - root - INFO - current CPU: 0.7860796583125
2023-11-28 20:34:19,732 - root - INFO - saving CPU 0.7860796583125 and max_pod 7
2023-11-28 20:34:19,808 - root - INFO - max_pod != pod_num, skipping closed loop
2023-11-28 20:34:19,809 - root - INFO - current max_pod: 7
2023-11-28 20:34:20,181 - root - INFO - scheduling job stress-ng --io 2 --vm 1 --vm-bytes 3G --timeout 168s
2023-11-28 20:34:20,256 - root - INFO - job scheduled
2023-11-28 20:34:24,736 - root - INFO - saving CPU 0.7860796583125 and max_pod 7
2023-11-28 20:34:25,860 - root - INFO - current CPU: 0.7860796583125
2023-11-28 20:34:26,097 - root - INFO - last jobst started 5.840902s ago, skipping closed loop
2023-11-28 20:34:26,098 - root - INFO - current max_pod: 7
2023-11-28 20:34:29,740 - root - INFO - saving CPU 0.7860796583125 and max_pod 7
2023-11-28 20:34:32,075 - root - INFO - current CPU: 0.9734250991875
2023-11-28 20:34:32,249 - root - INFO - setting max_pod: 6
2023-11-28 20:34:34,744 - root - INFO - saving CPU 0.9734250991875 and max_pod 6
2023-11-28 20:34:35,450 - root - INFO - current pod num: 7, max pod num: 6, job not scheduled
2023-11-28 20:34:38,111 - root - INFO - current CPU: 0.9734250991875
2023-11-28 20:34:38,348 - root - INFO - max_pod != pod_num, skipping closed loop
2023-11-28 20:34:38,348 - root - INFO - current max_pod: 6
2023-11-28 20:34:39,748 - root - INFO - saving CPU 0.9734250991875 and max_pod 6
2023-11-28 20:34:44,331 - root - INFO - current CPU: 0.999752229875
2023-11-28 20:34:44,530 - root - INFO - max_pod != pod_num, skipping closed loop
2023-11-28 20:34:44,531 - root - INFO - current max_pod: 6
2023-11-28 20:34:44,752 - root - INFO - saving CPU 0.999752229875 and max_pod 6
2023-11-28 20:34:49,764 - root - INFO - saving CPU 0.999752229875 and max_pod 6
2023-11-28 20:34:50,417 - root - INFO - current CPU: 0.999752229875
2023-11-28 20:34:50,661 - root - INFO - max_pod != pod_num, skipping closed loop
2023-11-28 20:34:50,661 - root - INFO - current max_pod: 6
2023-11-28 20:34:50,724 - root - INFO - current pod num: 7, max pod num: 6, job not scheduled
2023-11-28 20:34:54,768 - root - INFO - saving CPU 0.999752229875 and max_pod 6
2023-11-28 20:34:56,458 - root - INFO - current CPU: 0.999752229875
2023-11-28 20:34:56,671 - root - INFO - max_pod != pod_num, skipping closed loop
2023-11-28 20:34:56,671 - root - INFO - current max_pod: 6
2023-11-28 20:34:59,772 - root - INFO - saving CPU 0.999752229875 and max_pod 6
2023-11-28 20:35:02,611 - root - INFO - current CPU: 0.9996780903125
2023-11-28 20:35:02,786 - root - INFO - max_pod != pod_num, skipping closed loop
2023-11-28 20:35:02,787 - root - INFO - current max_pod: 6
2023-11-28 20:35:04,776 - root - INFO - saving CPU 0.9996780903125 and max_pod 6
2023-11-28 20:35:05,948 - root - INFO - current pod num: 7, max pod num: 6, job not scheduled
2023-11-28 20:35:08,563 - root - INFO - current CPU: 0.9996780903125
2023-11-28 20:35:08,718 - root - INFO - max_pod != pod_num, skipping closed loop
2023-11-28 20:35:08,718 - root - INFO - current max_pod: 6
2023-11-28 20:35:09,780 - root - INFO - saving CPU 0.9996780903125 and max_pod 6
2023-11-28 20:35:14,147 - root - INFO - current CPU: 0.9992415576249999
2023-11-28 20:35:14,356 - root - INFO - setting max_pod: 5
2023-11-28 20:35:14,785 - root - INFO - saving CPU 0.9992415576249999 and max_pod 5
2023-11-28 20:35:19,796 - root - INFO - saving CPU 0.9992415576249999 and max_pod 5
2023-11-28 20:35:19,812 - root - INFO - current CPU: 0.9992415576249999
2023-11-28 20:35:19,980 - root - INFO - max_pod != pod_num, skipping closed loop
2023-11-28 20:35:19,980 - root - INFO - current max_pod: 5
2023-11-28 20:35:21,095 - root - INFO - current pod num: 6, max pod num: 5, job not scheduled
2023-11-28 20:35:24,800 - root - INFO - saving CPU 0.9992415576249999 and max_pod 5
2023-11-28 20:35:25,466 - root - INFO - current CPU: 0.9992415576249999
2023-11-28 20:35:25,615 - root - INFO - max_pod != pod_num, skipping closed loop
2023-11-28 20:35:25,615 - root - INFO - current max_pod: 5
2023-11-28 20:35:29,804 - root - INFO - saving CPU 0.9992415576249999 and max_pod 5
2023-11-28 20:35:31,094 - root - INFO - current CPU: 0.879583623125
2023-11-28 20:35:31,255 - root - INFO - max_pod != pod_num, skipping closed loop
2023-11-28 20:35:31,255 - root - INFO - current max_pod: 5
2023-11-28 20:35:34,808 - root - INFO - saving CPU 0.879583623125 and max_pod 5
2023-11-28 20:35:36,252 - root - INFO - current pod num: 6, max pod num: 5, job not scheduled
2023-11-28 20:35:36,749 - root - INFO - current CPU: 0.879583623125
2023-11-28 20:35:36,919 - root - INFO - max_pod != pod_num, skipping closed loop
2023-11-28 20:35:36,919 - root - INFO - current max_pod: 5
2023-11-28 20:35:39,812 - root - INFO - saving CPU 0.879583623125 and max_pod 5
2023-11-28 20:35:42,395 - root - INFO - current CPU: 0.8517606684375
2023-11-28 20:35:42,546 - root - INFO - max_pod != pod_num, skipping closed loop
2023-11-28 20:35:42,546 - root - INFO - current max_pod: 5
2023-11-28 20:35:44,816 - root - INFO - saving CPU 0.8517606684375 and max_pod 5
2023-11-28 20:35:48,019 - root - INFO - current CPU: 0.8517606684375
2023-11-28 20:35:48,171 - root - INFO - max_pod != pod_num, skipping closed loop
2023-11-28 20:35:48,171 - root - INFO - current max_pod: 5
2023-11-28 20:35:49,820 - root - INFO - saving CPU 0.8517606684375 and max_pod 5
2023-11-28 20:35:51,416 - root - INFO - current pod num: 6, max pod num: 5, job not scheduled
2023-11-28 20:35:53,583 - root - INFO - current CPU: 0.8517606684375
2023-11-28 20:35:53,752 - root - INFO - max_pod != pod_num, skipping closed loop
2023-11-28 20:35:53,752 - root - INFO - current max_pod: 5
2023-11-28 20:35:54,824 - root - INFO - saving CPU 0.8517606684375 and max_pod 5
2023-11-28 20:35:59,139 - root - INFO - current CPU: 0.8354034180625
2023-11-28 20:35:59,440 - root - INFO - max_pod != pod_num, skipping closed loop
2023-11-28 20:35:59,441 - root - INFO - current max_pod: 5
2023-11-28 20:35:59,828 - root - INFO - saving CPU 0.8354034180625 and max_pod 5
2023-11-28 20:36:04,832 - root - INFO - saving CPU 0.8354034180625 and max_pod 5
2023-11-28 20:36:04,868 - root - INFO - current CPU: 0.8354034180625
2023-11-28 20:36:04,979 - root - INFO - max_pod != pod_num, skipping closed loop
2023-11-28 20:36:04,979 - root - INFO - current max_pod: 5
2023-11-28 20:36:06,542 - root - INFO - scheduling job stress-ng --io 3 --vm 3 --vm-bytes 1G --timeout 165s
2023-11-28 20:36:06,600 - root - INFO - job scheduled
2023-11-28 20:36:09,836 - root - INFO - saving CPU 0.8354034180625 and max_pod 5
2023-11-28 20:36:10,382 - root - INFO - current CPU: 0.8354034180625
2023-11-28 20:36:10,525 - root - INFO - last jobst started 3.925821s ago, skipping closed loop
2023-11-28 20:36:10,526 - root - INFO - current max_pod: 5
2023-11-28 20:36:14,840 - root - INFO - saving CPU 0.8354034180625 and max_pod 5
2023-11-28 20:36:15,983 - root - INFO - current CPU: 0.7170447196874999
2023-11-28 20:36:16,163 - root - INFO - max_pod != pod_num, skipping closed loop
2023-11-28 20:36:16,163 - root - INFO - current max_pod: 5
2023-11-28 20:36:19,844 - root - INFO - saving CPU 0.7170447196874999 and max_pod 5
2023-11-28 20:36:21,588 - root - INFO - current CPU: 0.7170447196874999
2023-11-28 20:36:21,745 - root - INFO - max_pod != pod_num, skipping closed loop
2023-11-28 20:36:21,745 - root - INFO - current max_pod: 5
2023-11-28 20:36:21,769 - root - INFO - scheduling job stress-ng --io 2 --timeout 155s
2023-11-28 20:36:21,829 - root - INFO - job scheduled
2023-11-28 20:36:24,848 - root - INFO - saving CPU 0.7170447196874999 and max_pod 5
2023-11-28 20:36:27,234 - root - INFO - current CPU: 0.7170447196874999
2023-11-28 20:36:27,386 - root - INFO - last jobst started 5.556654s ago, skipping closed loop
2023-11-28 20:36:27,386 - root - INFO - current max_pod: 5
2023-11-28 20:36:29,852 - root - INFO - saving CPU 0.7170447196874999 and max_pod 5
2023-11-28 20:36:32,843 - root - INFO - current CPU: 0.7113453315625
2023-11-28 20:36:32,988 - root - INFO - setting max_pod: 4
2023-11-28 20:36:34,856 - root - INFO - saving CPU 0.7113453315625 and max_pod 4
2023-11-28 20:36:37,001 - root - INFO - current pod num: 5, max pod num: 4, job not scheduled
2023-11-28 20:36:38,453 - root - INFO - current CPU: 0.7113453315625
2023-11-28 20:36:38,712 - root - INFO - max_pod != pod_num, skipping closed loop
2023-11-28 20:36:38,712 - root - INFO - current max_pod: 4
2023-11-28 20:36:39,860 - root - INFO - saving CPU 0.7113453315625 and max_pod 4
2023-11-28 20:36:44,163 - root - INFO - current CPU: 0.6682259268125
2023-11-28 20:36:44,300 - root - INFO - max_pod != pod_num, skipping closed loop
2023-11-28 20:36:44,300 - root - INFO - current max_pod: 4
2023-11-28 20:36:44,864 - root - INFO - saving CPU 0.6682259268125 and max_pod 4
2023-11-28 20:36:49,686 - root - INFO - current CPU: 0.6682259268125
2023-11-28 20:36:49,868 - root - INFO - saving CPU 0.6682259268125 and max_pod 4
2023-11-28 20:36:49,901 - root - INFO - setting max_pod: 4
2023-11-28 20:36:52,127 - root - INFO - current pod num: 4, max pod num: 4, job not scheduled
2023-11-28 20:36:54,872 - root - INFO - saving CPU 0.6682259268125 and max_pod 4
2023-11-28 20:36:55,323 - root - INFO - current CPU: 0.6682259268125
2023-11-28 20:36:55,480 - root - INFO - setting max_pod: 4
2023-11-28 20:36:59,876 - root - INFO - saving CPU 0.6682259268125 and max_pod 4
2023-11-28 20:37:00,841 - root - INFO - current CPU: 0.5734665473125
2023-11-28 20:37:00,950 - root - INFO - setting max_pod: 4
2023-11-28 20:37:04,902 - root - INFO - saving CPU 0.5734665473125 and max_pod 4
2023-11-28 20:37:06,279 - root - INFO - current CPU: 0.5734665473125
2023-11-28 20:37:06,453 - root - INFO - max_pod != pod_num, skipping closed loop
2023-11-28 20:37:06,453 - root - INFO - current max_pod: 4
2023-11-28 20:37:07,261 - root - INFO - scheduling job stress-ng --vm 1 --vm-bytes 4G --timeout 165s
2023-11-28 20:37:07,320 - root - INFO - job scheduled
2023-11-28 20:37:09,912 - root - INFO - saving CPU 0.5734665473125 and max_pod 4
2023-11-28 20:37:11,873 - root - INFO - current CPU: 0.5734665473125
2023-11-28 20:37:12,022 - root - INFO - last jobst started 4.701834s ago, skipping closed loop
2023-11-28 20:37:12,023 - root - INFO - current max_pod: 4
2023-11-28 20:37:14,916 - root - INFO - saving CPU 0.5734665473125 and max_pod 4
2023-11-28 20:37:17,418 - root - INFO - current CPU: 0.4292713255
2023-11-28 20:37:17,596 - root - INFO - max_pod != pod_num, skipping closed loop
2023-11-28 20:37:17,596 - root - INFO - current max_pod: 4
2023-11-28 20:37:19,922 - root - INFO - saving CPU 0.4292713255 and max_pod 4
2023-11-28 20:37:22,439 - root - INFO - scheduling job stress-ng --io 3 --vm 1 --vm-bytes 4G --timeout 159s
2023-11-28 20:37:22,495 - root - INFO - job scheduled
2023-11-28 20:37:23,048 - root - INFO - current CPU: 0.4292713255
2023-11-28 20:37:23,183 - root - INFO - last jobst started 0.688013s ago, skipping closed loop
2023-11-28 20:37:23,183 - root - INFO - current max_pod: 4
2023-11-28 20:37:24,928 - root - INFO - saving CPU 0.4292713255 and max_pod 4
2023-11-28 20:37:28,529 - root - INFO - current CPU: 0.300114234625
2023-11-28 20:37:28,673 - root - INFO - last jobst started 6.177508s ago, skipping closed loop
2023-11-28 20:37:28,673 - root - INFO - current max_pod: 4
2023-11-28 20:37:29,932 - root - INFO - saving CPU 0.300114234625 and max_pod 4
2023-11-28 20:37:34,015 - root - INFO - current CPU: 0.300114234625
2023-11-28 20:37:34,129 - root - INFO - setting max_pod: 5
2023-11-28 20:37:34,936 - root - INFO - saving CPU 0.300114234625 and max_pod 5
2023-11-28 20:37:37,629 - root - INFO - scheduling job stress-ng --io 3 --vm 2 --vm-bytes 2G --timeout 180s
2023-11-28 20:37:37,680 - root - INFO - job scheduled
2023-11-28 20:37:39,624 - root - INFO - current CPU: 0.300114234625
2023-11-28 20:37:39,776 - root - INFO - last jobst started 2.095975s ago, skipping closed loop
2023-11-28 20:37:39,776 - root - INFO - current max_pod: 5
2023-11-28 20:37:39,940 - root - INFO - saving CPU 0.300114234625 and max_pod 5
2023-11-28 20:37:44,944 - root - INFO - saving CPU 0.300114234625 and max_pod 5
2023-11-28 20:37:45,197 - root - INFO - current CPU: 0.32562586925
2023-11-28 20:37:45,357 - root - INFO - last jobst started 7.677356s ago, skipping closed loop
2023-11-28 20:37:45,358 - root - INFO - current max_pod: 5
2023-11-28 20:37:49,953 - root - INFO - saving CPU 0.32562586925 and max_pod 5
2023-11-28 20:37:50,722 - root - INFO - current CPU: 0.32562586925
2023-11-28 20:37:50,849 - root - INFO - setting max_pod: 7
2023-11-28 20:37:52,804 - root - INFO - scheduling job stress-ng --io 1 --vm 3 --vm-bytes 4G --timeout 156s
2023-11-28 20:37:52,860 - root - INFO - job scheduled
2023-11-28 20:37:54,956 - root - INFO - saving CPU 0.32562586925 and max_pod 7
2023-11-28 20:37:56,276 - root - INFO - current CPU: 0.32562586925
2023-11-28 20:37:56,439 - root - INFO - max_pod != pod_num, skipping closed loop
2023-11-28 20:37:56,440 - root - INFO - current max_pod: 7
2023-11-28 20:37:59,961 - root - INFO - saving CPU 0.32562586925 and max_pod 7
2023-11-28 20:38:01,891 - root - INFO - current CPU: 0.458217477625
2023-11-28 20:38:02,035 - root - INFO - max_pod != pod_num, skipping closed loop
2023-11-28 20:38:02,035 - root - INFO - current max_pod: 7
2023-11-28 20:38:04,968 - root - INFO - saving CPU 0.458217477625 and max_pod 7
2023-11-28 20:38:07,438 - root - INFO - current CPU: 0.458217477625
2023-11-28 20:38:07,586 - root - INFO - max_pod != pod_num, skipping closed loop
2023-11-28 20:38:07,586 - root - INFO - current max_pod: 7
2023-11-28 20:38:08,007 - root - INFO - scheduling job stress-ng --io 2 --vm 2 --vm-bytes 4G --timeout 154s
2023-11-28 20:38:08,080 - root - INFO - job scheduled
2023-11-28 20:38:09,972 - root - INFO - saving CPU 0.458217477625 and max_pod 7
2023-11-28 20:38:13,078 - root - INFO - current CPU: 0.5923434624375
2023-11-28 20:38:13,248 - root - INFO - last jobst started 5.168598s ago, skipping closed loop
2023-11-28 20:38:13,249 - root - INFO - current max_pod: 7
2023-11-28 20:38:14,976 - root - INFO - saving CPU 0.5923434624375 and max_pod 7
2023-11-28 20:38:18,704 - root - INFO - current CPU: 0.5923434624375
2023-11-28 20:38:18,936 - root - INFO - setting max_pod: 8
2023-11-28 20:38:19,980 - root - INFO - saving CPU 0.5923434624375 and max_pod 8
2023-11-28 20:38:23,266 - root - INFO - scheduling job stress-ng --vm 2 --vm-bytes 3G --timeout 151s
2023-11-28 20:38:23,328 - root - INFO - job scheduled
2023-11-28 20:38:24,422 - root - INFO - current CPU: 0.5923434624375
2023-11-28 20:38:24,606 - root - INFO - last jobst started 1.277739s ago, skipping closed loop
2023-11-28 20:38:24,607 - root - INFO - current max_pod: 8
2023-11-28 20:38:24,984 - root - INFO - saving CPU 0.5923434624375 and max_pod 8
2023-11-28 20:38:29,989 - root - INFO - saving CPU 0.5923434624375 and max_pod 8
2023-11-28 20:38:30,104 - root - INFO - current CPU: 0.75643958225
2023-11-28 20:38:30,304 - root - INFO - last jobst started 6.975428s ago, skipping closed loop
2023-11-28 20:38:30,304 - root - INFO - current max_pod: 8
2023-11-28 20:38:34,992 - root - INFO - saving CPU 0.75643958225 and max_pod 8
2023-11-28 20:38:35,879 - root - INFO - current CPU: 0.75643958225
2023-11-28 20:38:36,119 - root - INFO - setting max_pod: 9
2023-11-28 20:38:38,536 - root - INFO - scheduling job stress-ng --io 1 --vm 1 --vm-bytes 1G --timeout 154s
2023-11-28 20:38:38,616 - root - INFO - job scheduled
2023-11-28 20:38:39,996 - root - INFO - saving CPU 0.75643958225 and max_pod 9
2023-11-28 20:38:41,722 - root - INFO - current CPU: 0.75643958225
2023-11-28 20:38:41,933 - root - INFO - last jobst started 3.31691s ago, skipping closed loop
2023-11-28 20:38:41,933 - root - INFO - current max_pod: 9
2023-11-28 20:38:45,016 - root - INFO - saving CPU 0.75643958225 and max_pod 9
2023-11-28 20:38:47,674 - root - INFO - current CPU: 0.8635923294375
2023-11-28 20:38:47,895 - root - INFO - last jobst started 9.278954s ago, skipping closed loop
2023-11-28 20:38:47,895 - root - INFO - current max_pod: 9
2023-11-28 20:38:50,020 - root - INFO - saving CPU 0.8635923294375 and max_pod 9
2023-11-28 20:38:53,461 - root - INFO - current CPU: 0.8635923294375
2023-11-28 20:38:53,651 - root - INFO - setting max_pod: 9
2023-11-28 20:38:53,896 - root - INFO - current pod num: 9, max pod num: 9, job not scheduled
2023-11-28 20:38:55,024 - root - INFO - saving CPU 0.8635923294375 and max_pod 9
2023-11-28 20:38:59,085 - root - INFO - current CPU: 0.9551091269375
2023-11-28 20:38:59,289 - root - INFO - max_pod != pod_num, skipping closed loop
2023-11-28 20:38:59,289 - root - INFO - current max_pod: 9
2023-11-28 20:39:00,028 - root - INFO - saving CPU 0.9551091269375 and max_pod 9
2023-11-28 20:39:04,749 - root - INFO - current CPU: 0.9551091269375
2023-11-28 20:39:05,005 - root - INFO - max_pod != pod_num, skipping closed loop
2023-11-28 20:39:05,006 - root - INFO - current max_pod: 9
2023-11-28 20:39:05,032 - root - INFO - saving CPU 0.9551091269375 and max_pod 9
2023-11-28 20:39:09,076 - root - INFO - scheduling job stress-ng --io 1 --vm 2 --vm-bytes 2G --timeout 176s
2023-11-28 20:39:09,165 - root - INFO - job scheduled
2023-11-28 20:39:10,036 - root - INFO - saving CPU 0.9551091269375 and max_pod 9
2023-11-28 20:39:10,499 - root - INFO - current CPU: 0.9551091269375
2023-11-28 20:39:10,760 - root - INFO - max_pod != pod_num, skipping closed loop
2023-11-28 20:39:10,760 - root - INFO - current max_pod: 9
2023-11-28 20:39:15,040 - root - INFO - saving CPU 0.9551091269375 and max_pod 9
2023-11-28 20:39:16,256 - root - INFO - current CPU: 0.83386367025
2023-11-28 20:39:16,450 - root - INFO - max_pod != pod_num, skipping closed loop
2023-11-28 20:39:16,451 - root - INFO - current max_pod: 9
2023-11-28 20:39:20,046 - root - INFO - saving CPU 0.83386367025 and max_pod 9
2023-11-28 20:39:21,998 - root - INFO - current CPU: 0.83386367025
2023-11-28 20:39:22,223 - root - INFO - max_pod != pod_num, skipping closed loop
2023-11-28 20:39:22,223 - root - INFO - current max_pod: 9
2023-11-28 20:39:24,428 - root - INFO - scheduling job stress-ng --io 3 --vm 1 --vm-bytes 1G --timeout 175s
2023-11-28 20:39:24,506 - root - INFO - job scheduled
2023-11-28 20:39:25,052 - root - INFO - saving CPU 0.83386367025 and max_pod 9
2023-11-28 20:39:27,806 - root - INFO - current CPU: 0.8727765079375
2023-11-28 20:39:28,036 - root - INFO - last jobst started 3.53066s ago, skipping closed loop
2023-11-28 20:39:28,036 - root - INFO - current max_pod: 9
2023-11-28 20:39:30,056 - root - INFO - saving CPU 0.8727765079375 and max_pod 9
2023-11-28 20:39:33,617 - root - INFO - current CPU: 0.8727765079375
2023-11-28 20:39:33,815 - root - INFO - last jobst started 9.309091s ago, skipping closed loop
2023-11-28 20:39:33,815 - root - INFO - current max_pod: 9
2023-11-28 20:39:35,060 - root - INFO - saving CPU 0.8727765079375 and max_pod 9
2023-11-28 20:39:39,476 - root - INFO - current CPU: 0.8727765079375
2023-11-28 20:39:39,743 - root - INFO - setting max_pod: 9
2023-11-28 20:39:39,820 - root - INFO - current pod num: 9, max pod num: 9, job not scheduled
2023-11-28 20:39:40,064 - root - INFO - saving CPU 0.8727765079375 and max_pod 9
2023-11-28 20:39:44,794 - root - CRITICAL - error getting the CPU: Error: 500
2023-11-28 20:39:44,794 - root - CRITICAL - shutting down the controller
2023-11-28 20:39:45,068 - root - INFO - saving CPU 0.8727765079375 and max_pod 9
2023-11-28 20:39:50,072 - root - INFO - saving CPU 0.8727765079375 and max_pod 9
2023-11-28 20:39:55,066 - root - INFO - current pod num: 9, max pod num: 9, job not scheduled
2023-11-28 20:39:55,076 - root - INFO - saving CPU 0.8727765079375 and max_pod 9
2023-11-28 20:40:00,080 - root - INFO - saving CPU 0.8727765079375 and max_pod 9
2023-11-28 20:40:05,084 - root - INFO - saving CPU 0.8727765079375 and max_pod 9
2023-11-28 20:40:10,088 - root - INFO - saving CPU 0.8727765079375 and max_pod 9
2023-11-28 20:40:10,564 - root - INFO - scheduling job stress-ng --vm 1 --vm-bytes 2G --timeout 177s
2023-11-28 20:40:10,639 - root - INFO - job scheduled
2023-11-28 20:40:15,092 - root - INFO - saving CPU 0.8727765079375 and max_pod 9
2023-11-28 20:40:20,096 - root - INFO - saving CPU 0.8727765079375 and max_pod 9
2023-11-28 20:40:25,100 - root - INFO - saving CPU 0.8727765079375 and max_pod 9
2023-11-28 20:40:25,853 - root - INFO - scheduling job stress-ng --vm 1 --vm-bytes 2G --timeout 156s
2023-11-28 20:40:25,937 - root - INFO - job scheduled
2023-11-28 20:40:30,104 - root - INFO - saving CPU 0.8727765079375 and max_pod 9
2023-11-28 20:40:35,108 - root - INFO - saving CPU 0.8727765079375 and max_pod 9
2023-11-28 20:40:40,112 - root - INFO - saving CPU 0.8727765079375 and max_pod 9
2023-11-28 20:40:41,188 - root - INFO - scheduling job stress-ng --vm 1 --vm-bytes 4G --timeout 180s
2023-11-28 20:40:41,250 - root - INFO - job scheduled
2023-11-28 20:40:45,116 - root - INFO - saving CPU 0.8727765079375 and max_pod 9
2023-11-28 20:40:50,120 - root - INFO - saving CPU 0.8727765079375 and max_pod 9
2023-11-28 20:40:55,124 - root - INFO - saving CPU 0.8727765079375 and max_pod 9
2023-11-28 20:40:56,551 - root - INFO - scheduling job stress-ng --io 3 --vm 2 --vm-bytes 3G --timeout 178s
2023-11-28 20:40:56,617 - root - INFO - job scheduled
2023-11-28 20:41:00,140 - root - INFO - saving CPU 0.8727765079375 and max_pod 9
2023-11-28 20:41:05,164 - root - INFO - saving CPU 0.8727765079375 and max_pod 9
2023-11-28 20:41:10,168 - root - INFO - saving CPU 0.8727765079375 and max_pod 9
2023-11-28 20:41:11,832 - root - INFO - scheduling job stress-ng --io 1 --vm 1 --vm-bytes 2G --timeout 161s
2023-11-28 20:41:11,922 - root - INFO - job scheduled
2023-11-28 20:41:15,180 - root - INFO - saving CPU 0.8727765079375 and max_pod 9
2023-11-28 20:41:20,188 - root - INFO - saving CPU 0.8727765079375 and max_pod 9
2023-11-28 20:41:25,196 - root - INFO - saving CPU 0.8727765079375 and max_pod 9
